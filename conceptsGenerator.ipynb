{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "#DATA\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from skimage.util import view_as_blocks\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "#VIS\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/hamnamoieez/Desktop/PRDL/Distracted-Driver-Detection-Using-Deep-Learning/'\n",
    "train_imgs = base_dir + \"imgs/train/\"\n",
    "concept_path = base_dir + \"concept_images/\"\n",
    "concept_imgs = base_dir + \"concept_imgs/\"\n",
    "test_imgs =  os.path.join(base_dir, 'imgs/test/')\n",
    "weights_path = os.path.join(base_dir, 'weights/')\n",
    "class_mapper = {0: \"safe driving\",\n",
    "                1: \"texting - right\",\n",
    "                2: \"talking on the phone - right\",\n",
    "                3: \"texting - left\",\n",
    "                4: \"talking on the phone - left\",\n",
    "                5: \"operating the radio\",\n",
    "                6: \"drinking\",\n",
    "                7: \"reaching behind\",\n",
    "                8: \"hair and makeup\",\n",
    "                9: \"talking to passenger\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_path_holder = {}\n",
    "for img_path in glob(concept_path + \"*/*.jpg\"):\n",
    "    class_name = img_path.split(\"/\")[-2]\n",
    "    if class_name not in train_imgs_path_holder.keys():\n",
    "        train_imgs_path_holder[class_name] = []\n",
    "        train_imgs_path_holder[class_name].append(img_path)\n",
    "    else:\n",
    "        train_imgs_path_holder[class_name].append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hair and makeup during driving', 'drinking water during driving', 'talking on the phone during driving', 'operating the radio during driving', 'texting during driving', 'talking to passenger during driving', 'reaching behind during driving'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs_path_holder.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe_driving = train_imgs_path_holder[\"c0\"]\n",
    "# texting_right = train_imgs_path_holder[\"c1\"]\n",
    "talking_on_the_phone_right = train_imgs_path_holder[\"talking on the phone during driving\"]\n",
    "texting_left = train_imgs_path_holder[\"texting during driving\"]\n",
    "# talking_on_the_phone_left = train_imgs_path_holder[\"c4\"]\n",
    "\n",
    "operating_the_radio = train_imgs_path_holder[\"operating the radio during driving\"]\n",
    "drinking = train_imgs_path_holder[\"drinking water during driving\"]\n",
    "reaching_behind = train_imgs_path_holder[\"reaching behind during driving\"]\n",
    "hair_makeup = train_imgs_path_holder[\"hair and makeup during driving\"]\n",
    "talking_to_passenger = train_imgs_path_holder[\"talking to passenger during driving\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\"talking_on_the_phone_right\": talking_on_the_phone_right,\n",
    "          \"texting_left\": texting_left,\n",
    "          \"operating_the_radio\": operating_the_radio,\n",
    "          \"drinking\": drinking,\n",
    "          \"reaching_behind\": reaching_behind,\n",
    "          \"hair_makeup\": hair_makeup,\n",
    "          \"talking_to_passenger\": talking_to_passenger}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(img, patch_size, to_save_dir, img_id):\n",
    "    sub_level_counter = 0\n",
    "    patches = extract_patches_2d(img, patch_size)\n",
    "    total_patches = 0\n",
    "    for ix, patch in enumerate(patches):\n",
    "        if ix % 10 == 0:\n",
    "            sub_level_counter += 1\n",
    "            if sub_level_counter % 5 == 0:\n",
    "                plt.imsave(f'{to_save_dir}' + f'{img_id}_{total_patches}.jpg', patch)\n",
    "                total_patches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Class talking_on_the_phone_right!\n",
      "For Class texting_left!\n",
      "For Class operating_the_radio!\n",
      "For Class drinking!\n",
      "For Class reaching_behind!\n",
      "For Class hair_makeup!\n",
      "For Class talking_to_passenger!\n"
     ]
    }
   ],
   "source": [
    "take_random = 10\n",
    "patch_width, patch_height = 112, 112\n",
    "for cls_name, img_paths in classes.items():\n",
    "    print(f\"For Class {cls_name}!\")\n",
    "    to_save_dir = concept_imgs + f\"{cls_name}/\"\n",
    "    os.makedirs(to_save_dir, exist_ok = True)\n",
    "    random_paths = np.random.choice(img_paths, 10)\n",
    "    for img_id, img in enumerate(random_paths):\n",
    "        whole_img = cv2.resize(cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB), (224, 224))\n",
    "        img_height, img_width, _ = whole_img.shape\n",
    "        extract_patches(whole_img, (patch_width, patch_height), to_save_dir, img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_concept(concept_img, model):\n",
    "    test = image.load_img(concept_img, target_size=(112, 112))\n",
    "    img_array = image.img_to_array(test)\n",
    "    img_batch = np.expand_dims(img_array, axis=0)\n",
    "    img = np.vstack([img_batch])\n",
    "    feature_layer = keras.models.Model(inputs=model.input, outputs=model.get_layer('features').output)\n",
    "    prediction = feature_layer.predict(img)\n",
    "    preds = prediction.flatten()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(weights_path+'_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7822c47700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7822e18550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "concept_dict = {}\n",
    "for img_path in glob(concept_imgs + \"*/*.jpg\"):\n",
    "    img_name = img_path.split(\"/\")[-1]\n",
    "    concept_class = img_path.split(\"/\")[-2]\n",
    "    features = predict_concept(img_path, model)\n",
    "    if concept_class not in concept_dict.keys():\n",
    "        concept_dict[concept_class] = []\n",
    "        concept_dict[concept_class].append(features)\n",
    "    else:\n",
    "        concept_dict[concept_class].append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(concept_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('concepts.pickle', 'wb') as handle:\n",
    "    pickle.dump(concept_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('concepts.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['talking_to_passenger', 'talking_on_the_phone_left', 'hair_makeup', 'texting_right', 'talking_on_the_phone_right', 'operating_the_radio', 'drinking', 'texting_left', 'reaching_behind', 'safe_driving'])\n"
     ]
    }
   ],
   "source": [
    "print(b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred =np.argmax(prediction,axis=1)\n",
    "print(class_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talking to passenger\n"
     ]
    }
   ],
   "source": [
    "predicted_class_name = [class_mapper[k] for k in class_pred]\n",
    "print(predicted_class_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"c0\":\"safe_driving\",\n",
    "          \"c1\":\"texting_right\",\n",
    "          \"c2\":\"talking_on_the_phone_right\",\n",
    "          \"c3\":\"texting_left\",\n",
    "          \"c4\":\"talking_on_the_phone_left\",\n",
    "          \"c5\":\"operating_the_radio\",\n",
    "          \"c6\":\"drinking\",\n",
    "          \"c7\":\"reaching_behind\",\n",
    "          \"c8\":\"hair_makeup\",\n",
    "          \"c9\":\"talking_to_passenger\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dict = {}\n",
    "for img_path in glob(train_imgs + \"*/\"):\n",
    "    for img in glob(img_path +\"*.jpg\")[:30]:\n",
    "        img_name = img.split(\"/\")[-1]\n",
    "        img_class = class_map[img_path.split(\"/\")[-2]]\n",
    "        feat = predict_concept(img, model)\n",
    "        if img_class not in img_dict.keys():\n",
    "            img_dict[img_class] = []\n",
    "            img_dict[img_class].append(feat)\n",
    "        else:\n",
    "            img_dict[img_class].append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('images.pickle', 'wb') as handle:\n",
    "    pickle.dump(img_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
